name: Terminal-Bench

on:
  workflow_call:
    inputs:
      model_name:
        description: 'Model to use (e.g., anthropic:claude-sonnet-4-5)'
        required: false
        type: string
      thinking_level:
        description: 'Thinking level (off, low, medium, high)'
        required: false
        type: string
      dataset:
        description: 'Terminal-Bench dataset to use'
        required: false
        type: string
        default: 'terminal-bench-core==0.1.1'
      concurrency:
        description: 'Number of concurrent tasks (--n-concurrent)'
        required: false
        type: string
        default: '4'
      livestream:
        description: 'Enable livestream mode (verbose output to console)'
        required: false
        type: boolean
        default: false
      sample_size:
        description: 'Number of random tasks to run (empty = all tasks)'
        required: false
        type: string
      extra_args:
        description: 'Additional arguments to pass to terminal-bench'
        required: false
        type: string
      task_timeout:
        description: 'Per-task timeout in seconds (default: 1800 = 30 min)'
        required: false
        type: string
        default: '1800'
    secrets:
      ANTHROPIC_API_KEY:
        required: true
      OPENAI_API_KEY:
        required: true
  workflow_dispatch:
    inputs:
      dataset:
        description: 'Terminal-Bench dataset to use'
        required: false
        default: 'terminal-bench-core==0.1.1'
        type: string
      concurrency:
        description: 'Number of concurrent tasks (--n-concurrent)'
        required: false
        default: '4'
        type: string
      livestream:
        description: 'Enable livestream mode (verbose output to console)'
        required: false
        default: false
        type: boolean
      sample_size:
        description: 'Number of random tasks to run (empty = all tasks)'
        required: false
        type: string
      model_name:
        description: 'Model to use (e.g., anthropic:claude-sonnet-4-5, openai:gpt-5-codex)'
        required: false
        type: string
      thinking_level:
        description: 'Thinking level (off, low, medium, high)'
        required: false
        type: string
      extra_args:
        description: 'Additional arguments to pass to terminal-bench'
        required: false
        type: string
      task_timeout:
        description: 'Per-task timeout in seconds (default: 1800 = 30 min)'
        required: false
        type: string
        default: '1800'

jobs:
  benchmark:
    name: Run Terminal-Bench${{ inputs.model_name && format(' ({0})', inputs.model_name) || '' }}
    runs-on: ${{ github.repository_owner == 'coder' && 'depot-ubuntu-22.04-16' || 'ubuntu-latest' }}
    # Full suite (~80 tasks) at concurrency=4 takes ~60-90 minutes typically
    # Set 4-hour timeout to handle occasional API slowdowns while preventing infinite hangs
    # If consistently hitting this timeout, investigate task-level issues
    timeout-minutes: 240
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Required for git describe to find tags

      - uses: ./.github/actions/setup-cmux

      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Add uv to PATH
        run: echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Generate version file
        run: ./scripts/generate-version.sh

      - name: Build dist/ (skip icons - not needed for benchmark)
        run: make build-main build-preload

      - name: Run Terminal-Bench
        run: |
          echo "Starting Terminal-Bench run..."
          echo "Dataset: $TB_DATASET"
          echo "Concurrency: $TB_CONCURRENCY"
          echo "Task timeout: $TB_TIMEOUT seconds"
          echo "Sample size: ${TB_SAMPLE_SIZE:-all tasks}"
          echo ""
          
          # Run benchmark with output redirected to file (logs saved as artifact)
          # Only show progress indicator and final summary in console
          make benchmark-terminal 2>&1 | tee benchmark.log | grep -E "(Running Terminal-Bench|Using timeout|Selected task IDs|Results Summary|Resolved Trials|Unresolved Trials|Accuracy)" || true
          
          # Show last 50 lines if there was an error
          if [ ${PIPESTATUS[0]} -ne 0 ]; then
            echo ""
            echo "=== Benchmark failed, showing last 50 lines of output ==="
            tail -50 benchmark.log
          fi
        env:
          TB_DATASET: ${{ inputs.dataset }}
          TB_CONCURRENCY: ${{ inputs.concurrency }}
          TB_LIVESTREAM: ${{ inputs.livestream && '1' || '' }}
          TB_SAMPLE_SIZE: ${{ inputs.sample_size }}
          TB_TIMEOUT: ${{ inputs.task_timeout }}
          TB_ARGS: ${{ inputs.model_name && format('--agent-kwarg model_name={0} --agent-kwarg thinking_level={1} {2}', inputs.model_name, inputs.thinking_level, inputs.extra_args) || inputs.extra_args }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

      - name: Print results summary
        if: always()
        run: |
          echo "=== Terminal-Bench Results Summary ==="
          if [ -f "$(find runs -name 'results.json' 2>/dev/null | head -1)" ]; then
            RESULTS_FILE=$(find runs -name 'results.json' | head -1)
            echo "Results file: $RESULTS_FILE"
            echo ""
            echo "Summary statistics:"
            cat "$RESULTS_FILE" | jq '{n_resolved, n_unresolved, accuracy}' || echo "Failed to parse summary"
            echo ""
            echo "Per-task results:"
            cat "$RESULTS_FILE" | jq -r '.results[] | "\(.task_id): \(if .is_resolved then "✓ PASS" else "✗ FAIL" end) (\(.failure_mode // "unknown"))"' 2>/dev/null || echo "Failed to parse task details"
            echo ""
            
            # Check for timeout-related failures
            TOTAL=$(cat "$RESULTS_FILE" | jq '.results | length' 2>/dev/null || echo "0")
            TIMED_OUT=$(cat "$RESULTS_FILE" | jq '[.results[] | select(.failure_mode == "agent_timeout")] | length' 2>/dev/null || echo "0")
            if [ "$TIMED_OUT" -gt 0 ] && [ "$TOTAL" -gt 0 ]; then
              echo "⚠️  WARNING: $TIMED_OUT/$TOTAL tasks hit agent_timeout"
              echo "This may indicate tasks need more time or are genuinely stuck"
            fi
            
            # Full results for debugging
            echo ""
            echo "Full results.json:"
            cat "$RESULTS_FILE" | jq '.' || cat "$RESULTS_FILE"
          else
            echo "❌ No results.json found in runs/"
            if [ -d "runs" ]; then
              ls -laR runs/ || echo "Failed to list runs directory"
            else
              echo "runs/ directory does not exist"
            fi
          fi

      - name: Set artifact name
        if: always()
        id: artifact-name
        run: |
          if [ -n "${{ inputs.model_name }}" ]; then
            # Replace colons with hyphens for filesystem compatibility
            ARTIFACT_NAME="terminal-bench-results-$(echo "${{ inputs.model_name }}-${{ github.run_id }}" | tr ':' '-')"
          else
            ARTIFACT_NAME="terminal-bench-results-${{ github.run_id }}"
          fi
          echo "name=$ARTIFACT_NAME" >> $GITHUB_OUTPUT
          echo "Artifact name: $ARTIFACT_NAME"

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.artifact-name.outputs.name }}
          path: |
            runs/
            benchmark.log
          if-no-files-found: warn
          retention-days: 30


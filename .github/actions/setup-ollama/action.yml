name: Setup Ollama
description: Install Ollama and pull required models with caching
inputs:
  model:
    description: 'Ollama model to pull'
    required: false
    default: 'gpt-oss:20b'

runs:
  using: composite
  steps:
    - name: Cache Ollama
      id: cache-ollama
      uses: actions/cache@v4
      with:
        path: |
          /home/runner/.ollama
        key: ${{ runner.os }}-ollama-${{ inputs.model }}-v3

    - name: Install Ollama binary
      shell: bash
      run: |
        echo "Installing Ollama binary..."
        curl -fsSL https://ollama.com/install.sh | sh
        echo "Ollama binary installed"

    - name: Pull model (cache miss)
      if: steps.cache-ollama.outputs.cache-hit != 'true'
      shell: bash
      run: |
        echo "Cache miss - pulling model ${{ inputs.model }}..."
        
        # Start Ollama with model directory in user home
        export OLLAMA_MODELS="$HOME/.ollama/models"
        ollama serve > /tmp/ollama-setup.log 2>&1 &
        OLLAMA_PID=$!
        echo "$OLLAMA_PID" > /tmp/ollama-setup.pid
        
        # Wait for ready
        timeout 10 sh -c 'until curl -sf http://localhost:11434/api/tags > /dev/null 2>&1; do sleep 0.5; done' || {
          echo "Ollama failed to start"
          cat /tmp/ollama-setup.log
          kill $OLLAMA_PID 2>/dev/null || true
          exit 1
        }
        
        ollama pull ${{ inputs.model }}
        
        # Stop Ollama and ensure it's fully terminated
        echo "Stopping Ollama..."
        kill $OLLAMA_PID 2>/dev/null || true
        wait $OLLAMA_PID 2>/dev/null || true
        pkill -f "ollama serve" || true
        sleep 1
        
        echo "Model pulled successfully"
        echo "Copying models to cache location..."
        mkdir -p "$HOME/.ollama/models"
        sudo cp -r /usr/share/ollama/.ollama/models/* "$HOME/.ollama/models/" 
        sudo chown -R runner:docker "$HOME/.ollama/models"
        du -sh "$HOME/.ollama/models"
        echo "Models ready for caching"

    - name: Verify cache (cache hit)
      if: steps.cache-ollama.outputs.cache-hit == 'true'
      shell: bash
      run: |
        echo "Cache hit - models restored from cache"
        ls -lh "$HOME/.ollama/models" || echo "Warning: model directory not found"

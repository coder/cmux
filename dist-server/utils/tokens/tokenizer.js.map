{"version":3,"file":"tokenizer.js","sourceRoot":"","sources":["../../../src/utils/tokens/tokenizer.ts"],"names":[],"mappings":";AAAA;;GAEG;;;;;AAiEH,oDAyBC;AAKD,gDAGC;AAUD,0DAkCC;AA5ID,6CAAmE;AACnE,yCAAqC;AACrC,oDAA2B;AAC3B,mEAAkF;AAOlF;;;GAGG;AACH,MAAM,oBAAoB,GAAG,IAAI,GAAG,EAAoB,CAAC;AAEzD;;;;GAIG;AACH,MAAM,eAAe,GAAG,IAAI,oBAAQ,CAAiB;IACnD,GAAG,EAAE,MAAM,EAAE,6BAA6B;IAC1C,OAAO,EAAE,EAAE,GAAG,IAAI,GAAG,IAAI,EAAE,wBAAwB;IACnD,eAAe,EAAE,GAAG,EAAE;QACpB,sFAAsF;QACtF,OAAO,EAAE,CAAC;IACZ,CAAC;CACF,CAAC,CAAC;AAEH;;;GAGG;AACH,SAAS,0BAA0B,CAAC,SAAmB;IACrD,IAAI,CAAC,oBAAoB,CAAC,GAAG,CAAC,SAAS,CAAC,EAAE,CAAC;QACzC,oBAAoB,CAAC,GAAG,CAAC,SAAS,EAAE,IAAA,6BAAkB,EAAC,SAAS,CAAC,CAAC,CAAC;IACrE,CAAC;IACD,OAAO,oBAAoB,CAAC,GAAG,CAAC,SAAS,CAAE,CAAC;AAC9C,CAAC;AAED;;;GAGG;AACH,SAAS,iBAAiB,CAAC,IAAY,EAAE,UAAwB;IAC/D,MAAM,QAAQ,GAAG,gBAAK,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC;IACjC,MAAM,MAAM,GAAG,eAAe,CAAC,GAAG,CAAC,QAAQ,CAAC,CAAC;IAC7C,IAAI,MAAM,KAAK,SAAS,EAAE,CAAC;QACzB,OAAO,MAAM,CAAC;IAChB,CAAC;IAED,MAAM,KAAK,GAAG,UAAU,EAAE,CAAC;IAC3B,eAAe,CAAC,GAAG,CAAC,QAAQ,EAAE,KAAK,CAAC,CAAC;IACrC,OAAO,KAAK,CAAC;AACf,CAAC;AAED;;;;;GAKG;AACH,SAAgB,oBAAoB,CAAC,YAAoB;IACvD,mFAAmF;IACnF,8EAA8E;IAE9E,OAAO;QACL,IAAI,EAAE,UAAU;QAChB,WAAW,EAAE,CAAC,IAAY,EAAE,EAAE;YAC5B,OAAO,iBAAiB,CAAC,IAAI,EAAE,GAAG,EAAE;gBAClC,IAAI,CAAC;oBACH,wEAAwE;oBACxE,+CAA+C;oBAC/C,MAAM,OAAO,GAAG,0BAA0B,CAAC,QAAQ,CAAC,CAAC;oBACrD,MAAM,MAAM,GAAG,OAAO,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC;oBACpC,OAAO,MAAM,CAAC,MAAM,CAAC;gBACvB,CAAC;gBAAC,OAAO,KAAK,EAAE,CAAC;oBACf,8CAA8C;oBAC9C,OAAO,CAAC,KAAK,CACX,qEAAqE,EACrE,KAAK,CACN,CAAC;oBACF,OAAO,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;gBACpC,CAAC;YACH,CAAC,CAAC,CAAC;QACL,CAAC;KACF,CAAC;AACJ,CAAC;AAED;;GAEG;AACH,SAAgB,kBAAkB,CAAC,IAAa,EAAE,SAAoB;IACpE,MAAM,UAAU,GAAG,IAAI,CAAC,SAAS,CAAC,IAAI,CAAC,CAAC;IACxC,OAAO,SAAS,CAAC,WAAW,CAAC,UAAU,CAAC,CAAC;AAC3C,CAAC;AAED;;;;;;;GAOG;AACH,SAAgB,uBAAuB,CAAC,QAAgB,EAAE,WAAmB;IAC3E,IAAI,CAAC;QACH,iDAAiD;QACjD,MAAM,cAAc,GAAG,IAAA,mCAAiB,EAAC,WAAW,CAAC,CAAC;QACtD,IAAI,CAAC,cAAc,CAAC,QAAQ,CAAC,QAAQ,CAAC,EAAE,CAAC;YACvC,oCAAoC;YACpC,OAAO,CAAC,CAAC;QACX,CAAC;QAED,sBAAsB;QACtB,MAAM,WAAW,GAAG,IAAA,gCAAc,GAAE,CAAC;QACrC,MAAM,UAAU,GAAG,WAAW,CAAC,QAAQ,CAAC,CAAC;QAEzC,IAAI,CAAC,UAAU,EAAE,CAAC;YAChB,4CAA4C;YAC5C,OAAO,EAAE,CAAC;QACZ,CAAC;QAED,mDAAmD;QACnD,MAAM,UAAU,GAAG,IAAI,CAAC,SAAS,CAAC,UAAU,CAAC,CAAC;QAC9C,MAAM,SAAS,GAAG,oBAAoB,CAAC,WAAW,CAAC,CAAC;QACpD,OAAO,SAAS,CAAC,WAAW,CAAC,UAAU,CAAC,CAAC;IAC3C,CAAC;IAAC,MAAM,CAAC;QACP,8DAA8D;QAC9D,MAAM,aAAa,GAA2B;YAC5C,IAAI,EAAE,EAAE;YACR,SAAS,EAAE,EAAE;YACb,iBAAiB,EAAE,EAAE;YACrB,gBAAgB,EAAE,EAAE;YACpB,UAAU,EAAE,EAAE;YACd,aAAa,EAAE,EAAE;SAClB,CAAC;QACF,OAAO,aAAa,CAAC,QAAQ,CAAC,IAAI,EAAE,CAAC;IACvC,CAAC;AACH,CAAC","sourcesContent":["/**\n * Token calculation utilities for chat statistics\n */\n\nimport { encoding_for_model, type Tiktoken } from \"@dqbd/tiktoken\";\nimport { LRUCache } from \"lru-cache\";\nimport CRC32 from \"crc-32\";\nimport { getToolSchemas, getAvailableTools } from \"@/utils/tools/toolDefinitions\";\n\nexport interface Tokenizer {\n  name: string;\n  countTokens: (text: string) => number;\n}\n\n/**\n * Module-level cache for tiktoken encoders\n * Encoders are expensive to construct, so we cache and reuse them\n */\nconst tiktokenEncoderCache = new Map<string, Tiktoken>();\n\n/**\n * LRU cache for token counts by text checksum\n * Avoids re-tokenizing identical strings (system messages, tool definitions, etc.)\n * Key: CRC32 checksum of text, Value: token count\n */\nconst tokenCountCache = new LRUCache<number, number>({\n  max: 500000, // Max entries (safety limit)\n  maxSize: 16 * 1024 * 1024, // 16MB total cache size\n  sizeCalculation: () => {\n    // Each entry: ~8 bytes (key) + ~8 bytes (value) + ~32 bytes (LRU overhead) â‰ˆ 48 bytes\n    return 48;\n  },\n});\n\n/**\n * Get or create a cached tiktoken encoder for a given OpenAI model\n * This implements lazy initialization - encoder is only created on first use\n */\nfunction getOrCreateTiktokenEncoder(modelName: \"gpt-4o\"): Tiktoken {\n  if (!tiktokenEncoderCache.has(modelName)) {\n    tiktokenEncoderCache.set(modelName, encoding_for_model(modelName));\n  }\n  return tiktokenEncoderCache.get(modelName)!;\n}\n\n/**\n * Count tokens with caching via CRC32 checksum\n * Wraps the tokenization logic to avoid re-tokenizing identical strings\n */\nfunction countTokensCached(text: string, tokenizeFn: () => number): number {\n  const checksum = CRC32.str(text);\n  const cached = tokenCountCache.get(checksum);\n  if (cached !== undefined) {\n    return cached;\n  }\n\n  const count = tokenizeFn();\n  tokenCountCache.set(checksum, count);\n  return count;\n}\n\n/**\n * Get the appropriate tokenizer for a given model string\n *\n * @param modelString - Model identifier (e.g., \"anthropic:claude-opus-4-1\", \"openai:gpt-4\")\n * @returns Tokenizer interface with name and countTokens function\n */\nexport function getTokenizerForModel(_modelString: string): Tokenizer {\n  // Use GPT-4o tokenizer for Anthropic models - better approximation than char count\n  // Note: Not 100% accurate for Claude 3+, but close enough for cost estimation\n\n  return {\n    name: \"tiktoken\",\n    countTokens: (text: string) => {\n      return countTokensCached(text, () => {\n        try {\n          // Use o200k_base encoding for GPT-4o and newer models (GPT-5, o1, etc.)\n          // Encoder is cached and reused for performance\n          const encoder = getOrCreateTiktokenEncoder(\"gpt-4o\");\n          const tokens = encoder.encode(text);\n          return tokens.length;\n        } catch (error) {\n          // Log the error and fallback to approximation\n          console.error(\n            \"Failed to tokenize with js-tiktoken, falling back to approximation:\",\n            error\n          );\n          return Math.ceil(text.length / 4);\n        }\n      });\n    },\n  };\n}\n\n/**\n * Calculate token counts for serialized data (tool args/results)\n */\nexport function countTokensForData(data: unknown, tokenizer: Tokenizer): number {\n  const serialized = JSON.stringify(data);\n  return tokenizer.countTokens(serialized);\n}\n\n/**\n * Get estimated token count for tool definitions\n * These are the schemas sent to the API for each tool\n *\n * @param toolName The name of the tool (bash, file_read, web_search, etc.)\n * @param modelString The model string to get accurate tool definitions\n * @returns Estimated token count for the tool definition\n */\nexport function getToolDefinitionTokens(toolName: string, modelString: string): number {\n  try {\n    // Check if this tool is available for this model\n    const availableTools = getAvailableTools(modelString);\n    if (!availableTools.includes(toolName)) {\n      // Tool not available for this model\n      return 0;\n    }\n\n    // Get the tool schema\n    const toolSchemas = getToolSchemas();\n    const toolSchema = toolSchemas[toolName];\n\n    if (!toolSchema) {\n      // Tool not found, return a default estimate\n      return 40;\n    }\n\n    // Serialize the tool definition to estimate tokens\n    const serialized = JSON.stringify(toolSchema);\n    const tokenizer = getTokenizerForModel(modelString);\n    return tokenizer.countTokens(serialized);\n  } catch {\n    // Fallback to estimates if we can't get the actual definition\n    const fallbackSizes: Record<string, number> = {\n      bash: 65,\n      file_read: 45,\n      file_edit_replace: 70,\n      file_edit_insert: 50,\n      web_search: 50,\n      google_search: 50,\n    };\n    return fallbackSizes[toolName] || 40;\n  }\n}\n"]}
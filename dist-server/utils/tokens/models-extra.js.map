{"version":3,"file":"models-extra.js","sourceRoot":"","sources":["../../../src/utils/tokens/models-extra.ts"],"names":[],"mappings":";AAAA;;;;GAIG;;;AAmBU,QAAA,WAAW,GAA8B;IACpD,iDAAiD;IACjD,6BAA6B;IAC7B,4CAA4C;IAC5C,WAAW,EAAE;QACX,gBAAgB,EAAE,MAAM;QACxB,iBAAiB,EAAE,MAAM;QACzB,oBAAoB,EAAE,QAAQ,EAAE,+BAA+B;QAC/D,qBAAqB,EAAE,OAAO,EAAE,iCAAiC;QACjE,gBAAgB,EAAE,QAAQ;QAC1B,IAAI,EAAE,MAAM;QACZ,yBAAyB,EAAE,IAAI;QAC/B,eAAe,EAAE,IAAI;QACrB,kBAAkB,EAAE,IAAI;QACxB,wBAAwB,EAAE,IAAI;QAC9B,gBAAgB,EAAE,YAAY;QAC9B,mBAAmB,EAAE,CAAC,eAAe,CAAC;KACvC;CACF,CAAC","sourcesContent":["/**\n * Extra models not yet in LiteLLM's official models.json\n * This file is consulted as a fallback when a model is not found in the main file.\n * Models should be removed from here once they appear in the upstream LiteLLM repository.\n */\n\ninterface ModelData {\n  max_input_tokens: number;\n  max_output_tokens?: number;\n  input_cost_per_token: number;\n  output_cost_per_token: number;\n  cache_creation_input_token_cost?: number;\n  cache_read_input_token_cost?: number;\n  litellm_provider?: string;\n  mode?: string;\n  supports_function_calling?: boolean;\n  supports_vision?: boolean;\n  supports_reasoning?: boolean;\n  supports_response_schema?: boolean;\n  knowledge_cutoff?: string;\n  supported_endpoints?: string[];\n}\n\nexport const modelsExtra: Record<string, ModelData> = {\n  // GPT-5 Pro - Released October 6, 2025 at DevDay\n  // $15/M input, $120/M output\n  // Only available via OpenAI's Responses API\n  \"gpt-5-pro\": {\n    max_input_tokens: 400000,\n    max_output_tokens: 272000,\n    input_cost_per_token: 0.000015, // $15 per million input tokens\n    output_cost_per_token: 0.00012, // $120 per million output tokens\n    litellm_provider: \"openai\",\n    mode: \"chat\",\n    supports_function_calling: true,\n    supports_vision: true,\n    supports_reasoning: true,\n    supports_response_schema: true,\n    knowledge_cutoff: \"2024-09-30\",\n    supported_endpoints: [\"/v1/responses\"],\n  },\n};\n"]}